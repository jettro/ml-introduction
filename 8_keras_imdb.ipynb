{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting if a review is positive or negative based on the text\n",
    "A dataset is available that has labelled a lot of movie reviews as being positive or negative. ML models do not work with words, so we have to transform them somehow into numbers. The approach is to create an index where each word gets a unique number. Even with an amount of 1000 most used words, it is already possible to learn to recognise positive and negative texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data into chunks of test and training data. \n",
    "The labels are 1 and 0, the data number from 0 - 1000 if we have limited the amount of words to a 1000. After loading the data we also show some basic numbers that explain the data as well as some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS=1000 # only use top 1000 words\n",
    "INDEX_FROM=3   # word index offset\n",
    "\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "print(f'Shape of the training data {train_data.shape}')\n",
    "print(f'Shape of the training labels {train_labels.shape}')\n",
    "print(f'Shape of the test data {test_data.shape}')\n",
    "print(f'Shape of the test labels {test_labels.shape}')\n",
    "print(f'Number of words in first review is {len(train_data[0])}')\n",
    "print(f'Number of words in the second review is {len(train_data[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice about using text in machine learning\n",
    "Beware, this data set contains integers representing words. With the function below we can reverse using the index.\n",
    "Beware that we only took the top 1000 words. So some words cannot be reversed. These will be marked with a '?'\n",
    "\n",
    "To understand more about reverse and INDEX_FROM, check this post: https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "word_index = {k:(v+INDEX_FROM) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "print(f'Using the word index, we can find the number that represents the word: \"something\" is \"{word_index[\"something\"]}\"')\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "print(f'Using the reverse word index, we can find the word that represents the number: \"142\" is \"{reverse_word_index.get(142)}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_review = \" \".join([reverse_word_index.get(i, \"?\") for i in train_data[100]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=NUM_WORDS):\n",
    "    \"\"\"Creates one-hot-encoded vector, each row contains a 1 in a column if the matching index word is within the original sentence\"\"\"\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "         results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = vectorize_sequences(test_data)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model to check a new review\n",
    "In this part we will check with an exsiting review if the model can predict whether the review was positive or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(review_text):\n",
    "    words = review_text.split(\" \")\n",
    "    \n",
    "    word_index_ids = [word_index[\"<START>\"]]\n",
    "    UNKNOWN = word_index[\"<UNK>\"]\n",
    "    for a_word in words:\n",
    "        found_index = word_index.get(a_word, UNKNOWN)\n",
    "        word_index_ids.append(found_index if found_index < NUM_WORDS else UNKNOWN)\n",
    "    \n",
    "    return word_index_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_review = \"This is the best movie ever, I just love all the actors and the plot is better than ever.\"\n",
    "new_review = \"Wow, the worst ending ever, how can you do this, incredible. Nobody should ever look at this movie.\"\n",
    "\n",
    "# First step is encoding the text using the word index\n",
    "new_review_ids = text_to_index(new_review)\n",
    "print(new_review_ids)\n",
    "print(\" \".join([reverse_word_index.get(i, \"?\") for i in new_review_ids]))\n",
    "\n",
    "# vectorize the sentence\n",
    "datas = np.array([new_review_ids])\n",
    "new_review_vector = vectorize_sequences(datas)\n",
    "# Put the found vector through the model\n",
    "output = model.predict(new_review_vector)\n",
    "\n",
    "# Check the output\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
